{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is just a quick script that is able to load the files. Just using pandas can be tricky because of the newline characters in the text data. Here it is handled via the `parse_col` method.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "our_dataset_path = '.'\n",
    "\n",
    "posts_path = os.path.join(our_dataset_path, 'posts.csv')\n",
    "fact_checks_path = os.path.join(our_dataset_path, 'fact_checks.csv')\n",
    "fact_check_post_mapping_path = os.path.join(our_dataset_path, 'pairs.csv')\n",
    "\n",
    "for path in [posts_path, fact_checks_path, fact_check_post_mapping_path]:\n",
    "    assert os.path.isfile(path)\n",
    "\n",
    "# We need to apply t = t.replace('\\n', '\\\\n') for text fields before using `ast.literal_eval`.\n",
    "# `ast.literal_eval` has problems when there are new lines in the text, e.g.:\n",
    "# `ast.literal_eval('(\"\\n\")')` effectively tries to interpret the following code:\n",
    "\n",
    "# ```\n",
    "# (\"\n",
    "# \")\n",
    "# ```\n",
    "\n",
    "# This raises a SyntaxError exception. By escaping new lines we are able to force it to interpret it properly. There might\n",
    "# be some other way to do this more systematically, but it is a workable fix for now.\n",
    "\n",
    "parse_col = lambda s: ast.literal_eval(s.replace('\\n', '\\\\n')) if s else s\n",
    "\n",
    "df_fact_checks = pd.read_csv(fact_checks_path).fillna('').set_index('fact_check_id')\n",
    "for col in ['claim', 'instances', 'title']:\n",
    "    df_fact_checks[col] = df_fact_checks[col].apply(parse_col)\n",
    "\n",
    "\n",
    "df_posts = pd.read_csv(posts_path).fillna('').set_index('post_id')\n",
    "for col in ['instances', 'ocr', 'verdicts', 'text']:\n",
    "    df_posts[col] = df_posts[col].apply(parse_col)\n",
    "\n",
    "\n",
    "df_fact_check_post_mapping = pd.read_csv(fact_check_post_mapping_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tasks.json file and from it extract monlingual  posts_train, posts_dev, posts_test, fact_checks_train, fact_checks_dev, posts_dev\n",
    "import json\n",
    "with open('tasks.json') as f:\n",
    "    data_tasks = json.load(f)\n",
    "\n",
    "data_tasks = data_tasks['monolingual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tasks.json file and from it extract monlingual  posts_train, posts_dev, posts_test, fact_checks_train, fact_checks_dev, posts_dev\n",
    "import json\n",
    "with open('tasks.json') as f:\n",
    "    data_tasks = json.load(f)\n",
    "\n",
    "data_tasks = data_tasks['monolingual']\n",
    "\n",
    "#make them dataframes\n",
    "fact_checks_ = []\n",
    "posts__train = []\n",
    "posts__dev = []\n",
    "for key , value in data_tasks.items():\n",
    "    #append the values of the array to the list no the list itself\n",
    "    fact_checks_.extend(value['fact_checks'])\n",
    "    posts__train.extend(value['posts_train'])\n",
    "    posts__dev.extend(value['posts_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153743, 17016, 1891)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fact_checks_), len(posts__train), len(posts__dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('monolingual_predictions.json') as f:\n",
    "    monolingual_predictions_file = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fact_checks_ where fact_check_id is in fact_checks_ list . use fact_check_id as index\n",
    "df_fact_checks_ = df_fact_checks[df_fact_checks.index.isin(fact_checks_)]\n",
    "from sklearn.model_selection import train_test_split\n",
    "posts__train, posts__validate = train_test_split(posts__train, test_size=0.2, random_state=42)\n",
    "df_posts__train = df_posts[df_posts.index.isin(posts__train)]\n",
    "df_posts__validate = df_posts[df_posts.index.isin(posts__validate)]\n",
    "df_posts__dev = df_posts[df_posts.index.isin(posts__dev)]\n",
    "\n",
    "#inner join df_posts__train on index and df_fact_check_post_mapping on post_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instances</th>\n",
       "      <th>ocr</th>\n",
       "      <th>verdicts</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(1586139153.0, fb)]</td>\n",
       "      <td>[(!! WARNING !! A new thing circulating now. P...</td>\n",
       "      <td>[False information]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[(1657229688.0, fb)]</td>\n",
       "      <td>[(\"Un pueblo que elige corruptos, impostores, ...</td>\n",
       "      <td>[Partly false information]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[(1630863318.0, fb)]</td>\n",
       "      <td>[(#Baerbock bei #1 Live im Radio „Wir müssen d...</td>\n",
       "      <td>[False information]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(1627342737.0, fb)]</td>\n",
       "      <td>[(07/21/2021: Lab Alert: Changes to CDC RT-PCR...</td>\n",
       "      <td>[False information]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(1605538417.0, fb)]</td>\n",
       "      <td>[(07:14 f Vejam que MARAVILHA está acontecendo...</td>\n",
       "      <td>[Missing context]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    instances  \\\n",
       "post_id                         \n",
       "1        [(1586139153.0, fb)]   \n",
       "48       [(1657229688.0, fb)]   \n",
       "61       [(1630863318.0, fb)]   \n",
       "122      [(1627342737.0, fb)]   \n",
       "124      [(1605538417.0, fb)]   \n",
       "\n",
       "                                                       ocr  \\\n",
       "post_id                                                      \n",
       "1        [(!! WARNING !! A new thing circulating now. P...   \n",
       "48       [(\"Un pueblo que elige corruptos, impostores, ...   \n",
       "61       [(#Baerbock bei #1 Live im Radio „Wir müssen d...   \n",
       "122      [(07/21/2021: Lab Alert: Changes to CDC RT-PCR...   \n",
       "124      [(07:14 f Vejam que MARAVILHA está acontecendo...   \n",
       "\n",
       "                           verdicts text  \n",
       "post_id                                   \n",
       "1               [False information]       \n",
       "48       [Partly false information]       \n",
       "61              [False information]       \n",
       "122             [False information]       \n",
       "124               [Missing context]       "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts__dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#AsianPneumonia'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the ocr of the first post\n",
    "df_posts__dev.iloc[235]['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34340/2089324293.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__train['text'] = df_posts__train.apply(lambda x: x['text'][1] if x['text'] != '' else '', axis=1)\n",
      "/tmp/ipykernel_34340/2089324293.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__validate['text'] = df_posts__validate.apply(lambda x: x['text'][1] if x['text'] != '' else '', axis=1)\n",
      "/tmp/ipykernel_34340/2089324293.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__dev['text'] = df_posts__dev.apply(lambda x: x['text'][1] if x['text'] != '' else '', axis=1)\n",
      "/tmp/ipykernel_34340/2089324293.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__train['ocr'] = df_posts__train.apply(lambda x: x['ocr'][0][1] if len(x['ocr'])!=0 else '', axis=1)\n",
      "/tmp/ipykernel_34340/2089324293.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__validate['ocr'] = df_posts__validate.apply(lambda x: x['ocr'][0][1] if len(x['ocr'])!=0 else '', axis=1)\n",
      "/tmp/ipykernel_34340/2089324293.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__dev['ocr'] = df_posts__dev.apply(lambda x: x['ocr'][0][1] if len(x['ocr'])!=0 else '', axis=1)\n",
      "/tmp/ipykernel_34340/2089324293.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__train['data'] = df_posts__train.apply(lambda x: x['text'] if x['text'] != '' else x['ocr'], axis=1)\n",
      "/tmp/ipykernel_34340/2089324293.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__validate['data'] = df_posts__validate.apply(lambda x: x['text'] if x['text'] != '' else x['ocr'], axis=1)\n",
      "/tmp/ipykernel_34340/2089324293.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts__dev['data'] = df_posts__dev.apply(lambda x: x['text'] if x['text'] != '' else x['ocr'], axis=1)\n"
     ]
    }
   ],
   "source": [
    "#add a colums data to df_posts__train, df_posts__validate, df_posts__dev \n",
    "#where if text!= '' then data = text else data = ocr. if text==\" \" and ocr==\" \" then remove the row\n",
    "df_posts__train['text'] = df_posts__train.apply(lambda x: x['text'][1] if x['text'] != '' else '', axis=1)\n",
    "df_posts__validate['text'] = df_posts__validate.apply(lambda x: x['text'][1] if x['text'] != '' else '', axis=1)\n",
    "df_posts__dev['text'] = df_posts__dev.apply(lambda x: x['text'][1] if x['text'] != '' else '', axis=1)\n",
    "df_posts__train['ocr'] = df_posts__train.apply(lambda x: x['ocr'][0][1] if len(x['ocr'])!=0 else '', axis=1)\n",
    "df_posts__validate['ocr'] = df_posts__validate.apply(lambda x: x['ocr'][0][1] if len(x['ocr'])!=0 else '', axis=1)\n",
    "df_posts__dev['ocr'] = df_posts__dev.apply(lambda x: x['ocr'][0][1] if len(x['ocr'])!=0 else '', axis=1)\n",
    "df_posts__train['data'] = df_posts__train.apply(lambda x: x['text'] if x['text'] != '' else x['ocr'], axis=1)\n",
    "df_posts__train = df_posts__train[df_posts__train['data']!='']\n",
    "df_posts__validate['data'] = df_posts__validate.apply(lambda x: x['text'] if x['text'] != '' else x['ocr'], axis=1)\n",
    "df_posts__validate = df_posts__validate[df_posts__validate['data']!='']\n",
    "df_posts__dev['data'] = df_posts__dev.apply(lambda x: x['text'] if x['text'] != '' else x['ocr'], axis=1)\n",
    "df_posts__dev = df_posts__dev[df_posts__dev['data']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "simple_latin = string.ascii_lowercase + string.ascii_uppercase\n",
    "dirty_chars = string.digits + string.punctuation\n",
    "\n",
    "\n",
    "def is_clean_text(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Simple text cleaning method.\n",
    "    \"\"\"\n",
    "    dirty = (\n",
    "        len(text) < 25                                               # Short text\n",
    "        or\n",
    "        0.5 < sum(char in dirty_chars for char in text) / len(text)  # More than 50% dirty chars                                            \n",
    "    )\n",
    "    return not dirty\n",
    "\n",
    "\n",
    "url_regex = re.compile(\n",
    "    r'(?:^|(?<![\\w\\/\\.]))'\n",
    "    r'(?:(?:https?:\\/\\/|ftp:\\/\\/|www\\d{0,3}\\.))'\n",
    "    r'(?:\\S+(?::\\S*)?@)?' r'(?:'\n",
    "    r'(?!(?:10|127)(?:\\.\\d{1,3}){3})'\n",
    "    r'(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})'\n",
    "    r'(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})'\n",
    "    r'(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])'\n",
    "    r'(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}'\n",
    "    r'(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))'\n",
    "    r'|'\n",
    "    r'(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)'\n",
    "    r'(?:\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*'\n",
    "    r'(?:\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))' r'|' r'(?:(localhost))' r')'\n",
    "    r'(?::\\d{2,5})?'\n",
    "    r'(?:\\/[^\\)\\]\\}\\s]*)?',\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    return url_regex.sub('', text)\n",
    "\n",
    "\n",
    "# Source: https://gist.github.com/Nikitha2309/15337f4f593c4a21fb0965804755c41d\n",
    "emoji_regex = re.compile('['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002500-\\U00002BEF'  # chinese char\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        u'\\U0001f926-\\U0001f937'\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u'\\u2640-\\u2642'\n",
    "        u'\\u2600-\\u2B55'\n",
    "        u'\\u200d'\n",
    "        u'\\u23cf'\n",
    "        u'\\u23e9'\n",
    "        u'\\u231a'\n",
    "        u'\\ufe0f'  # dingbats\n",
    "        u'\\u3030'\n",
    "    ']+')\n",
    "\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return emoji_regex.sub('', text)\n",
    "\n",
    "\n",
    "sentence_stop_regex = re.compile('['\n",
    "    u'\\u002e' # full stop\n",
    "    u'\\u2026' # ellipsis\n",
    "    u'\\u061F' # arabic question mark\n",
    "    u'\\u06D4' # arabic full stop\n",
    "    u'\\u2022' # bullet point\n",
    "    u'\\u3002' # chinese period\n",
    "    u'\\u25CB' # white circle\n",
    "    '\\|'      # pipe\n",
    "']+')\n",
    "\n",
    "\n",
    "def replace_stops(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces some characters that are being used to end sentences. Used for sentence segmentation with sliding windows.\n",
    "    \"\"\"\n",
    "    return sentence_stop_regex.sub('.', text)\n",
    "\n",
    "\n",
    "whitespace_regex = re.compile(r'\\s+')\n",
    "\n",
    "\n",
    "def replace_whitespaces(text: str) -> str:\n",
    "    return whitespace_regex.sub(' ', text)\n",
    "\n",
    "\n",
    "def clean_ocr(ocr: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all lines that are shorter than 6 and have more than 50% `dirty_chars`.\n",
    "    \"\"\"\n",
    "    return '\\n'.join(\n",
    "        line\n",
    "        for line in ocr.split('\\n')\n",
    "        if len(line) > 5 and sum(char in dirty_chars for char in line) / len(line) < 0.5\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_twitter_picture_links(text):\n",
    "    \"\"\"\n",
    "    Replaces links to picture in twitter post only with 'pic'. \n",
    "    \"\"\"\n",
    "    return re.sub(r'pic.twitter.com/\\S+', 'pic', text)\n",
    "\n",
    "\n",
    "def clean_twitter_links(text):\n",
    "    \"\"\"\n",
    "    Replaces twitter links with 't.co'.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\S+//t.co/\\S+', 't.co', text)\n",
    "\n",
    "\n",
    "def remove_elongation(text):\n",
    "    \"\"\"\n",
    "    Replaces any occurrence of a string of consecutive identical non-space \n",
    "    characters (at least three in a row) with just one instance of that character.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'(\\S+)\\1{2,}', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Replace sentence stops with a period\n",
    "    text = replace_stops(text)\n",
    "\n",
    "    # Replace multiple whitespaces with a single space\n",
    "    text = replace_whitespaces(text)\n",
    "\n",
    "    # Remove lines shorter than 6 and with more than 50% dirty characters\n",
    "    text = clean_ocr(text)\n",
    "\n",
    "    # Replace Twitter picture links with 'pic'\n",
    "    text = clean_twitter_picture_links(text)\n",
    "\n",
    "    # Replace Twitter links with 't.co'\n",
    "    text = clean_twitter_links(text)\n",
    "\n",
    "    # Remove elongation (repeated characters)\n",
    "    text = remove_elongation(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# def preprocess_text(text):\n",
    "#     #lowercase\n",
    "    \n",
    "#     text = text.lower()\n",
    "#     #remove special characters\n",
    "#     text = re.sub(r'[^a-z\\s]', '', text)\n",
    "#     stemmer = PorterStemmer()  \n",
    "#     text = ' '.join([stemmer.stem(word) for word in text.split()]) \n",
    "#     return text\n",
    "\n",
    "df_posts__train['data'] = df_posts__train['data'].apply(preprocess_text)\n",
    "df_posts__validate['data'] = df_posts__validate['data'].apply(preprocess_text)\n",
    "df_posts__dev['data'] = df_posts__dev['data'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instances</th>\n",
       "      <th>ocr</th>\n",
       "      <th>verdicts</th>\n",
       "      <th>text</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(1586139153.0, fb)]</td>\n",
       "      <td>!! WARNING !! A new thing circulating now. Peo...</td>\n",
       "      <td>[False information]</td>\n",
       "      <td></td>\n",
       "      <td>!! warning !! a new thing circulating now. peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[(1657229688.0, fb)]</td>\n",
       "      <td>\"A people who choose corrupt, impostors, thiev...</td>\n",
       "      <td>[Partly false information]</td>\n",
       "      <td></td>\n",
       "      <td>\"a people who choose corrupt, impostors, thiev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[(1630863318.0, fb)]</td>\n",
       "      <td>#Baerbock at #1 Live im radio “We have to adva...</td>\n",
       "      <td>[False information]</td>\n",
       "      <td></td>\n",
       "      <td>#baerbock at #1 live im radio “we have to adva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(1627342737.0, fb)]</td>\n",
       "      <td>07/21/2021: Lab Alert: Changes to CDC RT-PCR f...</td>\n",
       "      <td>[False information]</td>\n",
       "      <td></td>\n",
       "      <td>07/21/2021: lab alert: changes to cdc rt-pcr f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(1605538417.0, fb)]</td>\n",
       "      <td>07:14 pm Look how WONDERFUL it is happening in...</td>\n",
       "      <td>[Missing context]</td>\n",
       "      <td></td>\n",
       "      <td>07:14 pm look how wonderful it is happening in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    instances  \\\n",
       "post_id                         \n",
       "1        [(1586139153.0, fb)]   \n",
       "48       [(1657229688.0, fb)]   \n",
       "61       [(1630863318.0, fb)]   \n",
       "122      [(1627342737.0, fb)]   \n",
       "124      [(1605538417.0, fb)]   \n",
       "\n",
       "                                                       ocr  \\\n",
       "post_id                                                      \n",
       "1        !! WARNING !! A new thing circulating now. Peo...   \n",
       "48       \"A people who choose corrupt, impostors, thiev...   \n",
       "61       #Baerbock at #1 Live im radio “We have to adva...   \n",
       "122      07/21/2021: Lab Alert: Changes to CDC RT-PCR f...   \n",
       "124      07:14 pm Look how WONDERFUL it is happening in...   \n",
       "\n",
       "                           verdicts text  \\\n",
       "post_id                                    \n",
       "1               [False information]        \n",
       "48       [Partly false information]        \n",
       "61              [False information]        \n",
       "122             [False information]        \n",
       "124               [Missing context]        \n",
       "\n",
       "                                                      data  \n",
       "post_id                                                     \n",
       "1        !! warning !! a new thing circulating now. peo...  \n",
       "48       \"a people who choose corrupt, impostors, thiev...  \n",
       "61       #baerbock at #1 live im radio “we have to adva...  \n",
       "122      07/21/2021: lab alert: changes to cdc rt-pcr f...  \n",
       "124      07:14 pm look how wonderful it is happening in...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts__dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_check_post_mapping = df_fact_check_post_mapping.set_index('post_id')\n",
    "df_posts__train = df_posts__train.join(df_fact_check_post_mapping, how='inner')\n",
    "\n",
    "#inner join df_posts__validate on index and df_fact_check_post_mapping on post_id\n",
    "df_posts__validate = df_posts__validate.join(df_fact_check_post_mapping, how='inner')\n",
    "# in column claim of df_fact_checks_, extract the second element of the list and replace the column claim with this element\n",
    "df_fact_checks_[\"claim\"] = df_fact_checks_[\"claim\"].apply(lambda x: x[1])\n",
    "df_fact_checks_.drop(columns=['instances', 'title'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "claim    !! He drinks imported mineral water, Evian boy...\n",
       "Name: 12, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_checks_.iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!! warning !! a new thing circulating now. peo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"a people who choose corrupt, impostors, thiev...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#baerbock at #1 live im radio “we have to adva...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07/21/2021: lab alert: changes to cdc rt-pcr f...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07:14 pm look how wonderful it is happening in...</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  post_id\n",
       "0  !! warning !! a new thing circulating now. peo...        1\n",
       "1  \"a people who choose corrupt, impostors, thiev...       48\n",
       "2  #baerbock at #1 live im radio “we have to adva...       61\n",
       "3  07/21/2021: lab alert: changes to cdc rt-pcr f...      122\n",
       "4  07:14 pm look how wonderful it is happening in...      124"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts__dev.drop(columns=['instances', 'ocr', 'verdicts', 'text'], inplace=True)\n",
    "df_posts__dev['post_id'] = df_posts__dev.index\n",
    "df_posts__dev.reset_index(drop=True, inplace=True)\n",
    "df_posts__dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>data</th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>! brazen vaccination fake by markus söder! it'...</td>\n",
       "      <td>87108</td>\n",
       "      <td>Markus Söder faked his vaccination.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>\"blessed are those persecuted by me cause \"the...</td>\n",
       "      <td>80729</td>\n",
       "      <td>The photo shows nuns arrested for participatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>\"cigarette smoking does not cause cancer.\" -ce...</td>\n",
       "      <td>33862</td>\n",
       "      <td>CDC said cigarette smoking doesn't cause cance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>\"eat what you want on easter, the sacrifice is...</td>\n",
       "      <td>50769</td>\n",
       "      <td>The Uruguayan priest \"Gordo\" Verde said to \"ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>\"the need will die force people to bend!\" that...</td>\n",
       "      <td>150241</td>\n",
       "      <td>Wolfgang Schäuble said: \"Necessity will force ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                               data  fact_check_id  \\\n",
       "0        0  ! brazen vaccination fake by markus söder! it'...          87108   \n",
       "1        4  \"blessed are those persecuted by me cause \"the...          80729   \n",
       "2        5  \"cigarette smoking does not cause cancer.\" -ce...          33862   \n",
       "3        6  \"eat what you want on easter, the sacrifice is...          50769   \n",
       "4        9  \"the need will die force people to bend!\" that...         150241   \n",
       "\n",
       "                                               claim  \n",
       "0                Markus Söder faked his vaccination.  \n",
       "1  The photo shows nuns arrested for participatin...  \n",
       "2  CDC said cigarette smoking doesn't cause cance...  \n",
       "3  The Uruguayan priest \"Gordo\" Verde said to \"ea...  \n",
       "4  Wolfgang Schäuble said: \"Necessity will force ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts__train.drop(columns=['instances', 'verdicts','ocr','text'], inplace=True)\n",
    "#add a column claim to df_posts__train from df_fact_checks_\n",
    "df_posts__train = df_posts__train.join(df_fact_checks_, on='fact_check_id', how='inner')\n",
    "#add a colmun post_id to df_posts__train from index\n",
    "df_posts__train['post_id'] = df_posts__train.index\n",
    "#reset index of df_posts__train\n",
    "df_posts__train.reset_index(drop=True, inplace=True)\n",
    "#make post od as first column\n",
    "df_posts__train = df_posts__train[['post_id','data','fact_check_id', 'claim']]\n",
    "\n",
    "# in column ocr of df_posts__train, extract the second element of the list and replace the column ocr with this element\n",
    "df_posts__train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>data</th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>\"actually, he's a damn sight better than any o...</td>\n",
       "      <td>93524</td>\n",
       "      <td>New Zealand opposition leader Judith Collins p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>speech by pedro castillo, it was based on the ...</td>\n",
       "      <td>56968</td>\n",
       "      <td>Felipe VI said that Pedro Castillo's speech is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>\"we must be solidarity with those who least ha...</td>\n",
       "      <td>148668</td>\n",
       "      <td>Wado de Pedro: \"We must be in Solidarity with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>\"environmentalists\" say fracking is evil this ...</td>\n",
       "      <td>153628</td>\n",
       "      <td>environmentalists hypocritical over stance on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>\"out of work, everything will be forbidden wal...</td>\n",
       "      <td>52407</td>\n",
       "      <td>In \"1984\", Orwell wrote that \"having fun, sing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                               data  fact_check_id  \\\n",
       "0        2  \"actually, he's a damn sight better than any o...          93524   \n",
       "1        7  speech by pedro castillo, it was based on the ...          56968   \n",
       "2        8  \"we must be solidarity with those who least ha...         148668   \n",
       "3       15  \"environmentalists\" say fracking is evil this ...         153628   \n",
       "4       17  \"out of work, everything will be forbidden wal...          52407   \n",
       "\n",
       "                                               claim  \n",
       "0  New Zealand opposition leader Judith Collins p...  \n",
       "1  Felipe VI said that Pedro Castillo's speech is...  \n",
       "2  Wado de Pedro: \"We must be in Solidarity with ...  \n",
       "3  environmentalists hypocritical over stance on ...  \n",
       "4  In \"1984\", Orwell wrote that \"having fun, sing...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do same for df_posts__validate\n",
    "df_posts__validate.drop(columns=['instances', 'verdicts','ocr','text'], inplace=True)\n",
    "df_posts__validate = df_posts__validate.join(df_fact_checks_, on='fact_check_id', how='inner')\n",
    "df_posts__validate['post_id'] = df_posts__validate.index\n",
    "df_posts__validate.reset_index(drop=True, inplace=True)\n",
    "df_posts__validate = df_posts__validate[['post_id','data', 'fact_check_id', 'claim']]\n",
    "\n",
    "df_posts__validate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_checks_['fact_check_id'] = df_fact_checks_.index\n",
    "df_fact_checks_.reset_index(drop=True, inplace=True)\n",
    "df_fact_checks_['claim'] = df_fact_checks_['claim'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>fact_check_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>are avocados good for you?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can animals have headaches?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can we help prevent alzheimer's with diet?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do any benefits of alcohol outweigh the risks?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>does acupuncture work for headaches?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             claim  fact_check_id\n",
       "0                       are avocados good for you?              0\n",
       "1                      can animals have headaches?              1\n",
       "2       can we help prevent alzheimer's with diet?              2\n",
       "3   do any benefits of alcohol outweigh the risks?              3\n",
       "4             does acupuncture work for headaches?              4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_checks_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>data</th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>! brazen vaccination fake by markus söder! it'...</td>\n",
       "      <td>87108</td>\n",
       "      <td>Markus Söder faked his vaccination.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>\"blessed are those persecuted by me cause \"the...</td>\n",
       "      <td>80729</td>\n",
       "      <td>The photo shows nuns arrested for participatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>\"cigarette smoking does not cause cancer.\" -ce...</td>\n",
       "      <td>33862</td>\n",
       "      <td>CDC said cigarette smoking doesn't cause cance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>\"eat what you want on easter, the sacrifice is...</td>\n",
       "      <td>50769</td>\n",
       "      <td>The Uruguayan priest \"Gordo\" Verde said to \"ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>\"the need will die force people to bend!\" that...</td>\n",
       "      <td>150241</td>\n",
       "      <td>Wolfgang Schäuble said: \"Necessity will force ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                               data  fact_check_id  \\\n",
       "0        0  ! brazen vaccination fake by markus söder! it'...          87108   \n",
       "1        4  \"blessed are those persecuted by me cause \"the...          80729   \n",
       "2        5  \"cigarette smoking does not cause cancer.\" -ce...          33862   \n",
       "3        6  \"eat what you want on easter, the sacrifice is...          50769   \n",
       "4        9  \"the need will die force people to bend!\" that...         150241   \n",
       "\n",
       "                                               claim  \n",
       "0                Markus Söder faked his vaccination.  \n",
       "1  The photo shows nuns arrested for participatin...  \n",
       "2  CDC said cigarette smoking doesn't cause cance...  \n",
       "3  The Uruguayan priest \"Gordo\" Verde said to \"ea...  \n",
       "4  Wolfgang Schäuble said: \"Necessity will force ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts__train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15996 entries, 0 to 15995\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   post_id        15996 non-null  int64 \n",
      " 1   data           15996 non-null  object\n",
      " 2   fact_check_id  15996 non-null  int64 \n",
      " 3   claim          15996 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 500.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_posts__train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List, Dict\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, k1: float = 1.2, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.bm25 = None\n",
    "        \n",
    "    def fit(self, df_fact_checks: pd.DataFrame):\n",
    "        \"\"\"Fit the BM25 model with fact-check claims.\"\"\"\n",
    "        self.fact_check_ids = df_fact_checks['fact_check_id'].tolist()\n",
    "        tokenized_claims = [claim.split() for claim in df_fact_checks_['claim']]\n",
    "        self.bm25 = BM25Okapi(tokenized_claims, k1=self.k1, b=self.b)\n",
    "\n",
    "    def retrieve_top_k(self, post_data: str, k) -> List[int]:\n",
    "        \"\"\"Retrieve the top k fact_check_ids for a given post.\"\"\"\n",
    "        tokenized_post = post_data.split()\n",
    "        scores = self.bm25.get_scores(tokenized_post)\n",
    "        top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "        return top_k_indices\n",
    "\n",
    "\n",
    "class BM25Evaluator:\n",
    "    def __init__(self, retriever: BM25Retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def evaluate_success_at_10(self, df_posts__validate: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate the success@10 metric and return the average score and top 10 fact_check_ids.\"\"\"\n",
    "        success_at_10 = 0\n",
    "        top_10_results = {}\n",
    "\n",
    "        for _, row in df_posts__validate.iterrows():\n",
    "            post_id = row['post_id']\n",
    "            # correct_fact_id = row['fact_check_id']\n",
    "            retrieved_fact_ids = self.retriever.retrieve_top_k(row['data'], k=10)\n",
    "\n",
    "            # Check if the correct fact ID is in the top 10 retrieved results\n",
    "            if row['fact_check_id'] in retrieved_fact_ids:\n",
    "                success_at_10 += 1\n",
    "\n",
    "        #     # Store the retrieved fact IDs for this post\n",
    "            top_10_results[post_id] = retrieved_fact_ids\n",
    "\n",
    "        # Calculate the average success@10 score\n",
    "        avg_success_at_10 = success_at_10 / len(df_posts__validate)\n",
    "        \n",
    "        # return {'average_score': avg_success_at_10, 'top_10_results': top_10_results}\n",
    "        return top_10_results , avg_success_at_10\n",
    "\n",
    "# bm25_retriever = BM25Retriever(k1=1.2, b=0.75)\n",
    "# bm25_retriever.fit(df_fact_checks_)\n",
    "# with open('BM25API.pkl', 'wb') as f:\n",
    "#     pickle.dump(bm25_retriever, f)\n",
    "# Initialize retriever with BM25 param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573456674\n"
     ]
    }
   ],
   "source": [
    "#save model to file \n",
    "import pickle\n",
    "\n",
    "\n",
    "bm25_retriever_loaded = pickle.load(open('BM25API.pkl', 'rb'))\n",
    "# Initialize evaluator and evaluate success@10\n",
    "evaluator = BM25Evaluator(bm25_retriever_loaded)\n",
    "results ,top10 = evaluator.evaluate_success_at_10(df_posts__validate)\n",
    "top10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Success@10 Score: 0.632335235\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load your data\n",
    "# df_fact_checks_ = pd.DataFrame(...) # Contains columns 'claim' and 'fact_check_id'\n",
    "# df_posts__train = pd.DataFrame(...) # Contains columns 'post_id', 'data', and 'fact_check_id'\n",
    "\n",
    "# Initialize the sentence encoder model (e.g., \"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Path to store the FAISS index file\n",
    "index_file_path = 'fact_check_index.faiss'\n",
    "id_to_fact_check = {}  # Dictionary to store the mapping from fact check index to fact_check_id\n",
    "\n",
    "# Step 1: Check if the FAISS index already exists, otherwise create it\n",
    "if not os.path.exists(index_file_path):\n",
    "    # Encode claims and build the vector database\n",
    "    claim_embeddings = model.encode(df_fact_checks_['claim'].tolist(), convert_to_tensor=False)\n",
    "    claim_embeddings = np.array(claim_embeddings).astype('float32')\n",
    "    \n",
    "    # Step 2: Initialize FAISS index\n",
    "    dimension = claim_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Add claim embeddings to index\n",
    "    index.add(claim_embeddings)\n",
    "    \n",
    "    # Save fact_check_id metadata in id_to_fact_check dictionary\n",
    "    id_to_fact_check = {i: fact_id for i, fact_id in enumerate(df_fact_checks_['fact_check_id'].tolist())}\n",
    "    \n",
    "    # Save the FAISS index and metadata\n",
    "    faiss.write_index(index, index_file_path)\n",
    "    with open('fact_check_id_mapping.pkl', 'wb') as f:\n",
    "        pickle.dump(id_to_fact_check, f)\n",
    "else:\n",
    "    # Load the FAISS index and metadata if already created\n",
    "    index = faiss.read_index(index_file_path)\n",
    "    with open('fact_check_id_mapping.pkl', 'rb') as f:\n",
    "        id_to_fact_check = pickle.load(f)\n",
    "\n",
    "# Step 3: Prepare to evaluate using Success@10\n",
    "top_10_results = {}  # Dictionary to store results for each post\n",
    "\n",
    "# Encode each post as query\n",
    "for _, post in df_posts__validate.iterrows():\n",
    "    post_id = post['post_id']\n",
    "    correct_fact_id = post['fact_check_id']\n",
    "    \n",
    "    # Get post embedding\n",
    "    post_embedding = model.encode([post['data']], convert_to_tensor=False)\n",
    "    post_embedding = np.array(post_embedding).astype('float32')\n",
    "    \n",
    "    # Step 4: Retrieve top 10 fact_check_ids for the post\n",
    "    _, top_10_indices = index.search(post_embedding, k=10)\n",
    "    top_10_fact_check_ids = [id_to_fact_check[idx] for idx in top_10_indices[0]]\n",
    "    top_10_results[post_id] = top_10_fact_check_ids\n",
    "    \n",
    "    # Check if the correct fact_check_id is in the top 10 results\n",
    "    top_10_results[post_id] = 1 if correct_fact_id in top_10_fact_check_ids else 0\n",
    "\n",
    "# Step 5: Calculate the average score\n",
    "average_score = np.mean(list(top_10_results.values()))\n",
    "\n",
    "# Results\n",
    "print(\"Average Success@10 Score:\", average_score)\n",
    "# print(\"Top 10 Fact Check IDs for each post:\", top_10_results)\n",
    "\n",
    "# Return the results\n",
    "# average_score, top_10_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Success@10 Score: 0.653462345\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load your data\n",
    "# df_fact_checks_ = pd.DataFrame(...) # Contains columns 'claim' and 'fact_check_id'\n",
    "# df_posts__validate = pd.DataFrame(...) # Contains columns 'post_id', 'data', and 'fact_check_id'\n",
    "\n",
    "# Initialize the sentence encoder model (e.g., \"all-mpnet-base-v2\") and move to GPU\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "model.to('cuda')  # Move to GPU for faster encoding\n",
    "\n",
    "# Path to store the FAISS index file\n",
    "index_file_path = 'fact_check_index_mpnet.faiss'\n",
    "id_to_fact_check = {}  # Dictionary to store the mapping from fact check index to fact_check_id\n",
    "\n",
    "# Step 1: Check if the FAISS index already exists, otherwise create it\n",
    "if not os.path.exists(index_file_path):\n",
    "    # Encode claims and build the vector database\n",
    "    claim_embeddings = model.encode(\n",
    "        df_fact_checks_['claim'].tolist(), \n",
    "        convert_to_tensor=False,\n",
    "        device='cuda'  # Use GPU for encoding\n",
    "    )\n",
    "    claim_embeddings = np.array(claim_embeddings).astype('float32')\n",
    "\n",
    "    # Step 2: Initialize FAISS index on GPU\n",
    "    dimension = claim_embeddings.shape[1]\n",
    "    res = faiss.StandardGpuResources()  # Initialize GPU resources\n",
    "    index_flat = faiss.IndexFlatL2(dimension)  # Base CPU index\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index_flat)  # Move to GPU\n",
    "    \n",
    "    # Add claim embeddings to the FAISS index\n",
    "    index.add(claim_embeddings)\n",
    "    \n",
    "    # Save fact_check_id metadata in id_to_fact_check dictionary\n",
    "    id_to_fact_check = {i: fact_id for i, fact_id in enumerate(df_fact_checks_['fact_check_id'].tolist())}\n",
    "    \n",
    "    # Save the FAISS index and metadata\n",
    "    faiss.write_index(faiss.index_gpu_to_cpu(index), index_file_path)  # Save as CPU index\n",
    "    with open('fact_check_id_mapping_mpnet.pkl', 'wb') as f:\n",
    "        pickle.dump(id_to_fact_check, f)\n",
    "else:\n",
    "    # Load the FAISS index and metadata if already created\n",
    "    index = faiss.read_index(index_file_path)\n",
    "    with open('fact_check_id_mapping_mpnet.pkl', 'rb') as f:\n",
    "        id_to_fact_check = pickle.load(f)\n",
    "\n",
    "    # Move the index back to GPU\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "# Step 3: Prepare to evaluate using Success@10\n",
    "top_10_results = {}  # Dictionary to store results for each post\n",
    "\n",
    "# Encode each post as query\n",
    "for _, post in df_posts__validate.iterrows():\n",
    "    post_id = post['post_id']\n",
    "    correct_fact_id = post['fact_check_id']\n",
    "    \n",
    "    # Get post embedding using GPU\n",
    "    post_embedding = model.encode(\n",
    "        [post['data']], \n",
    "        convert_to_tensor=False,\n",
    "        device='cuda'  # Use GPU for encoding\n",
    "    )\n",
    "    post_embedding = np.array(post_embedding).astype('float32')\n",
    "    \n",
    "    # Step 4: Retrieve top 10 fact_check_ids for the post using FAISS GPU index\n",
    "    _, top_10_indices = index.search(post_embedding, k=10)\n",
    "    top_10_fact_check_ids = [id_to_fact_check[idx] for idx in top_10_indices[0]]\n",
    "    top_10_results[post_id] = top_10_fact_check_ids\n",
    "    \n",
    "    # Check if the correct fact_check_id is in the top 10 results\n",
    "    top_10_results[post_id] = 1 if correct_fact_id in top_10_fact_check_ids else 0\n",
    "\n",
    "# Step 5: Calculate the average score\n",
    "average_score = np.mean(list(top_10_results.values()))\n",
    "\n",
    "# Results\n",
    "print(\"Average Success@10 Score:\", average_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suyash/suyash/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load your data\n",
    "# df_fact_checks_ = pd.DataFrame(...) # Contains columns 'claim' and 'fact_check_id'\n",
    "# df_posts__train = pd.DataFrame(...) # Contains columns 'post_id', 'data', and 'fact_check_id'\n",
    "\n",
    "# Initialize the sentence encoder model (e.g., \"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Path to store the FAISS index file\n",
    "index_file_path = 'fact_check_index.faiss'\n",
    "# id_to_fact_check = {}  # Dictionary to store the mapping from fact check index to fact_check_id\n",
    "index = faiss.read_index(index_file_path)\n",
    "with open('fact_check_id_mapping.pkl', 'rb') as f:\n",
    "        id_to_fact_check = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Claim: images of red sky in china are real.\n",
      "Top 10 Retrieved Claims:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['images of red sky in china are real.',\n",
       " 'the sky of china is red in a strange phenomenon',\n",
       " 'picture of the sky in china turning red.',\n",
       " 'panic at the airport in shanghai - and nobody reports about it? a video shows people in protective suits trying to hold back a crowd. a facebook post about it says: \"the fake media don\\'t show that!\" (archived here) what the user means by \"fakemedien\", he clarifies in another post: \"world, rtl, ntv\".',\n",
       " '“so now the fake news @nytimes is tracing the coronavirus origins back to europe, not china.\"',\n",
       " 'image of newspaper showing trump bombing south china sea',\n",
       " 'the sky turned blood red in china',\n",
       " \"news broadcaster admiring china's response to deadly flooding in henan\",\n",
       " 'a marine tsunami hits china during the past few hours in a live broadcast on cnn on the social networking site facebook',\n",
       " 'the video shows recent flooding at an airport in china']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"The fake news of red sky in china is floating on twitter\"    \n",
    "post_embedding = model.encode(query, convert_to_tensor=False)\n",
    "post_embedding = np.array(post_embedding).astype('float32')\n",
    "\n",
    "# Step 4: Retrieve top 10 documents for the query\n",
    "distances, indices = index.search(post_embedding.reshape(1, -1), 10)\n",
    "\n",
    "top_claims = [df_fact_checks_.iloc[idx]['claim'] for idx in indices[0]]\n",
    "actual_claim = \"images of red sky in china are real.\"\n",
    "print(\"Actual Claim:\", actual_claim)\n",
    "print(\"Top 10 Retrieved Claims:\", )\n",
    "top_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #save the claim_embeddings\n",
    "with open('_before_faiss_claim_embeddings_mpnet.pkl', 'wb') as f:\n",
    "        pickle.dump(claim_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/jainit/miniconda3/envs/mistral/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Dual Encoder Model with Separate Encoders\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, query_model_name, doc_model_name=None):\n",
    "        \"\"\"\n",
    "        query_model_name: Pre-trained model for query encoder.\n",
    "        doc_model_name: Pre-trained model for document encoder (defaults to query_model_name if None).\n",
    "        \"\"\"\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.query_encoder = AutoModel.from_pretrained(query_model_name)\n",
    "        self.doc_encoder = AutoModel.from_pretrained(doc_model_name or query_model_name)\n",
    "\n",
    "    def encode_query(self, query_inputs):\n",
    "        \"\"\"\n",
    "        Encode query using the query encoder.\n",
    "        \"\"\"\n",
    "        query_embeddings = self.query_encoder(**query_inputs).last_hidden_state[:, 0, :]\n",
    "        query_embeddings = nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
    "        return query_embeddings\n",
    "\n",
    "    def encode_document(self, doc_inputs):\n",
    "        \"\"\"\n",
    "        Encode document using the document encoder.\n",
    "        \"\"\"\n",
    "        doc_embeddings = self.doc_encoder(**doc_inputs).last_hidden_state[:, 0, :]\n",
    "        doc_embeddings = nn.functional.normalize(doc_embeddings, p=2, dim=1)\n",
    "        return doc_embeddings\n",
    "\n",
    "    def forward(self, query_inputs, doc_inputs):\n",
    "        \"\"\"\n",
    "        Forward pass to encode both query and document.\n",
    "        \"\"\"\n",
    "        query_embeddings = self.encode_query(query_inputs)\n",
    "        doc_embeddings = self.encode_document(doc_inputs)\n",
    "        return query_embeddings, doc_embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualEncoder(\n",
       "  (query_encoder): MPNetModel(\n",
       "    (embeddings): MPNetEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): MPNetEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_attention_bias): Embedding(32, 12)\n",
       "    )\n",
       "    (pooler): MPNetPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (doc_encoder): MPNetModel(\n",
       "    (embeddings): MPNetEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): MPNetEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_attention_bias): Embedding(32, 12)\n",
       "    )\n",
       "    (pooler): MPNetPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "device = \"cuda\"\n",
    "# dataset = TripletDataset(training_data, tokenizer)\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = DualEncoder(model_name)\n",
    "model.load_state_dict(torch.load(\"best_dual_encoder_mpnet.pt\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize the model\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "def predict_with_sentences(model, posts, df_fact_checks_, tokenizer, batch_size=32, device='cuda', top_k=10):\n",
    "    # Move model to the correct device\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Step 1: Compute fact-check embeddings in batches\n",
    "    idx_to_fact_check_id = {idx: fact_check_id for idx, fact_check_id in enumerate(df_fact_checks_.index.tolist())}\n",
    "    idx_to_fact_check_sentence = {idx: claim for idx, claim in enumerate(df_fact_checks_['claim'].tolist())}\n",
    "    fact_check_texts = df_fact_checks_['claim'].tolist()\n",
    "    all_facts_embeddings = []\n",
    "    \n",
    "    fact_check_loader = DataLoader(\n",
    "        fact_check_texts,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: tokenizer(x, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(fact_check_loader, desc=\"Encoding Fact-Checks\"):\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            embeddings = model.encode_document(batch).cpu()\n",
    "            all_facts_embeddings.append(embeddings)\n",
    "    \n",
    "    all_facts_embeddings = torch.cat(all_facts_embeddings, dim=0)  # Shape: [num_facts, embedding_dim]\n",
    "    \n",
    "    # Normalize fact-check embeddings\n",
    "    all_facts_embeddings = all_facts_embeddings / torch.norm(all_facts_embeddings, dim=1, keepdim=True)\n",
    "\n",
    "    # Step 2: Compute post embeddings and retrieve top-k fact-checks\n",
    "    top_k_results = {}\n",
    "    post_loader = DataLoader(\n",
    "        posts,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: tokenizer(x, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(post_loader, desc=\"Predicting Posts\")):\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            post_embeddings = model.encode_query(batch).cpu()\n",
    "\n",
    "            # Normalize post embeddings\n",
    "            post_embeddings = post_embeddings / torch.norm(post_embeddings, dim=1, keepdim=True)\n",
    "\n",
    "            # Compute cosine similarity: [batch_size, num_facts]\n",
    "            similarities = torch.matmul(post_embeddings, all_facts_embeddings.T)\n",
    "\n",
    "            # Extract top-k indices and map to fact-check IDs and sentences\n",
    "            for j, similarity_scores in enumerate(similarities):\n",
    "                post_idx = i * batch_size + j  # Index of the current post in the input list\n",
    "                top_k_indices = similarity_scores.topk(top_k).indices.tolist()\n",
    "                top_k_fact_checks = [\n",
    "                    {\n",
    "                        \"fact_check_id\": idx_to_fact_check_id[idx],\n",
    "                        \"sentence\": idx_to_fact_check_sentence[idx]\n",
    "                    }\n",
    "                    for idx in top_k_indices\n",
    "                ]\n",
    "                top_k_results[post_idx] = top_k_fact_checks\n",
    "\n",
    "    return top_k_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Fact-Checks: 100%|██████████| 4805/4805 [03:23<00:00, 23.60it/s]\n",
      "Predicting Posts: 100%|██████████| 1/1 [00:00<00:00, 35.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 0:\n",
      "  Rank 1:\n",
      "    Fact-check ID: 137588\n",
      "    Sentence: the sky of china is red in a strange phenomenon\n",
      "  Rank 2:\n",
      "    Fact-check ID: 59015\n",
      "    Sentence: images of red sky in china are real.\n",
      "  Rank 3:\n",
      "    Fact-check ID: 139338\n",
      "    Sentence: picture of the sky in china turning red.\n",
      "  Rank 4:\n",
      "    Fact-check ID: 110349\n",
      "    Sentence: the sky turned blood red in china\n",
      "  Rank 5:\n",
      "    Fact-check ID: 67110\n",
      "    Sentence: water in a river in eastern china turned red in february 2020\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example inputs\n",
    "\n",
    "posts = [\"The fake news of red sky in china is floating on twitter\"]  # Replace with actual post texts\n",
    "\n",
    "# Predict top-k fact-checks for the given posts\n",
    "predictions = predict_with_sentences(model, posts, df_fact_checks_, tokenizer, top_k=5)\n",
    "\n",
    "# Output predictions in a nicely formatted way\n",
    "for post_idx, top_k_fact_checks in predictions.items():\n",
    "    print(f\"Post {post_idx}:\")\n",
    "    for rank, fact in enumerate(top_k_fact_checks, start=1):\n",
    "        print(f\"  Rank {rank}:\")\n",
    "        print(f\"    Fact-check ID: {fact['fact_check_id']}\")\n",
    "        print(f\"    Sentence: {fact['sentence']}\")\n",
    "    print(\"-\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained COlbert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suyash/suyash/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel, PreTrainedModel,PretrainedConfig\n",
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "class ColBERTConfig(PretrainedConfig):\n",
    "    model_type = \"ColBERT\"\n",
    "    bert_model: str\n",
    "    compression_dim: int = 768\n",
    "    dropout: float = 0.0\n",
    "    return_vecs: bool = False\n",
    "    trainable: bool = True\n",
    "\n",
    "class ColBERT(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    ColBERT model from: https://arxiv.org/pdf/2004.12832.pdf\n",
    "    We use a dot-product instead of cosine per term (slightly better)\n",
    "    \"\"\"\n",
    "    config_class = ColBERTConfig\n",
    "    base_model_prefix = \"bert_model\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg) -> None:\n",
    "        super().__init__(cfg)\n",
    "        \n",
    "        self.bert_model = AutoModel.from_pretrained(cfg.bert_model)\n",
    "\n",
    "        for p in self.bert_model.parameters():\n",
    "            p.requires_grad = cfg.trainable\n",
    "        #if document_embeddings.pkl exists, load ii in self in self.document_embeddings\n",
    "        if hasattr(self, 'document_embeddings'):\n",
    "            self.document_vecs = torch.load('document_embeddings.pkl')\n",
    "        \n",
    "\n",
    "        self.compressor = torch.nn.Linear(self.bert_model.config.hidden_size, cfg.compression_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                query: Dict[str, torch.LongTensor],\n",
    "                document: Dict[str, torch.LongTensor],fact_check_id):\n",
    "\n",
    "        query_vecs = self.forward_representation(query)\n",
    "        # if documenT embedding of the fact_check_id exists, use it, else compute it\n",
    "        if(self.document_vecs is not None and fact_check_id in self.document_vecs):\n",
    "            document_vecs = self.document_vecs[fact_check_id]\n",
    "        else:\n",
    "            document_vecs = self.forward_representation(document)\n",
    "\n",
    "        score = self.forward_aggregation(query_vecs,document_vecs,query[\"attention_mask\"],document[\"attention_mask\"])\n",
    "        return score\n",
    "\n",
    "    def forward_representation(self,\n",
    "                               tokens,\n",
    "                               sequence_type=None) -> torch.Tensor:\n",
    "        \n",
    "        vecs = self.bert_model(**tokens)[0] # assuming a distilbert model here\n",
    "        vecs = self.compressor(vecs)\n",
    "        # truncate the sequence to the maximum length the model was trained on\n",
    "        # if encoding only, zero-out the mask values so we can compress storage\n",
    "        if sequence_type == \"doc_encode\" or sequence_type == \"query_encode\": \n",
    "            vecs = vecs * tokens[\"tokens\"][\"mask\"].unsqueeze(-1)\n",
    "\n",
    "        return vecs\n",
    "\n",
    "    def forward_aggregation(self,query_vecs, document_vecs,query_mask,document_mask):\n",
    "        \n",
    "        # create initial term-x-term scores (dot-product)\n",
    "        score = torch.bmm(query_vecs, document_vecs.transpose(2,1))\n",
    "\n",
    "        # mask out padding on the doc dimension (mask by -1000, because max should not select those, setting it to 0 might select them)\n",
    "        exp_mask = document_mask.bool().unsqueeze(1).expand(-1,score.shape[1],-1)\n",
    "        score[~exp_mask] = - 10000\n",
    "\n",
    "        # max pooling over document dimension\n",
    "        score = score.max(-1).values\n",
    "\n",
    "        # mask out paddding query values\n",
    "        score[~(query_mask.bool())] = 0\n",
    "\n",
    "        # sum over query values\n",
    "        score = score.sum(-1)\n",
    "\n",
    "        return score\n",
    "\n",
    "#\n",
    "# init the model & tokenizer (using the distilbert tokenizer)\n",
    "#\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # honestly not sure if that is the best way to go, but it works :)\n",
    "model = ColBERT.from_pretrained(\"sebastian-hofstaetter/colbert-distilbert-margin_mse-T2-msmarco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store document embeddings in a dictionary with key as post_id and value as the document embedding  store in a file\n",
    "import pickle\n",
    "\n",
    "document_embeddings = {}\n",
    "def embed_docs_and_append(text, post_id,filename):\n",
    "    document_input = tokenizer(text,truncation=True, max_length=512,return_tensors=\"pt\")\n",
    "    #truncate the input to 512 tokens\n",
    "    document_vecs = model.forward_representation(document_input)\n",
    "\n",
    "    with open(filename, 'ab') as f:\n",
    "        pickle.dump({post_id:document_vecs}, f)\n",
    "\n",
    "i=0\n",
    "print(len(df_fact_checks_))\n",
    "for _, claim in df_fact_checks_.iterrows():\n",
    "    embed_docs_and_append(claim['claim'], claim['fact_check_id'],'document_embeddings.pkl')\n",
    "    # print(i)\n",
    "    # i+=1\n",
    "import pickle\n",
    "with open('document_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(document_embeddings, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_posts__validate_short = df_posts__validate[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64234534\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# nested for loop for every query and every document\n",
    "answer_dict = {}\n",
    "for _, query in df_posts__validate.iterrows():\n",
    "    query_text = query['data']\n",
    "    # print(query_text)\n",
    "    query_input = tokenizer(query_text)\n",
    "    query_input.input_ids += [103] * 8\n",
    "    query_input.attention_mask += [1] * 8\n",
    "    query_input[\"input_ids\"] = torch.LongTensor(query_input.input_ids).unsqueeze(0)\n",
    "    query_input[\"attention_mask\"] = torch.LongTensor(query_input.attention_mask).unsqueeze(0)\n",
    "\n",
    "    scores = []\n",
    "    for _, document in df_fact_checks_.iterrows():\n",
    "        \n",
    "        document_text = document['claim']\n",
    "        # print(document_text)\n",
    "        document_input = tokenizer(document_text,return_tensors=\"pt\")\n",
    "        # get the score\n",
    "        score = model.forward(query_input, document_input).squeeze(0)\n",
    "        scores.append((document['fact_check_id'], float(score)))\n",
    "\n",
    "        # print(score)\n",
    "    #give dictionary of key query_id and mapping to a list of top 10 document_id\n",
    "    top_10 = sorted(scores, key=lambda x: x[1], reverse=True)[:10]\n",
    "    answer_dict[query['post_id']] = [fact_check_id for fact_check_id, _ in top_10]\n",
    "\n",
    "save_path = 'answer_dict.json'\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(answer_dict, f)\n",
    "\n",
    "# check sucess @10 for the validation set\n",
    "def success_at_10(answer_dict, df_posts__validate):\n",
    "    success = 0\n",
    "    for query_id, top_10 in answer_dict.items():\n",
    "            if df_posts__validate.loc[\"post_id\"]['fact_check_id'] in top_10:\n",
    "                success += 1\n",
    "    return success/len(df_posts__validate)\n",
    "\n",
    "ans=success_at_10(answer_dict, df_posts__validate)\n",
    "print(ans)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suyash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
